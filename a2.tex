\documentclass{article}
\usepackage{graphicx} % For figures
\usepackage{float} % For figure positioning
\usepackage{subcaption} % For side-by-side figures
\usepackage{amsmath} % For math environments align, aligned, gather, gathered, multline
\usepackage{multirow}
\usepackage{pifont} % for checkmarks and crosses


\begin{document}
    \raggedright
    \title{STAT380: Assignment 1  \\}
    \author{Vivienne Crowe ID:40071153}
    \date{25/1/22}
    \maketitle
    \section{Part A}
    \subsection{}

    Let $x_i$ denote the $i$-th row of the matrix$\bf{X}$

    \begin{center}
        \begin{tabular}{ |c||c|c|c|  }
            \hline
            \multicolumn{4}{|c|}{kNN}\\
            \hline
            i & $||x_i-x_0||_2$ & $\in N_0$  when $k = 1$ &$\in N_0$  when $k = 3$\\
            \hline
            1 & 1 &\ding{51} & \ding{51}\\
            2 & $\sqrt{2}$ & \ding{55} & \ding{51}\\
            3 & $\sqrt{5}$ & \ding{55} & \ding{55}\\
            4 & $\sqrt{3}$ & \ding{55} & \ding{51}\\
            5 & $3$ & \ding{55} & \ding{55}\\
            6 & $\sqrt{5}$ & \ding{55} & \ding{55}\\
            \hline
           \end{tabular}
        \end{center}

        Hence, for $k=1$  $\hat{y}_0 = y_1 = 0$, and when $k=3$  $\hat{y}_0 = \frac{1}{1}(0+5-5) = 0$.

    \subsection{}

    \begin{center}
    \begin{tabular}{ |c||c|c|c|  }
        \hline
        \multicolumn{4}{|c|}{Parzen Window}\\
        \hline
        i & $||x_i-x_0||_2$ & $\in W_0$ when $D = 2$ &$\in W_0$ when $D = 3$\\
        \hline
        1 & 1 &\ding{51} & \ding{51}\\
        2 & $\sqrt{2}$ & \ding{51} & \ding{51}\\
        3 & $\sqrt{5}$ & \ding{55} & \ding{51}\\
        4 & $\sqrt{3}$ & \ding{51} & \ding{51}\\
        5 & $3$ & \ding{55} & \ding{51}\\
        6 & $\sqrt{5}$ & \ding{55} & \ding{51}\\
        \hline
       \end{tabular}
    \end{center}

    When $D=2$ the Parzen Window prediction is $\hat{y}_0 = \frac{1}{3}(0+5-5) = 0$.
    When $D=3$ the Parzen Window prediction is $\hat{y}_0 = \frac{1}{6}(0+5+6-5+4+1) = \frac{11}{6}$

    \subsection{A.3}

    First note that the expectation of $MSE_{test}$, 
     is independent of the number of values in the test set.
     \[\begin{aligned}
        & E \left[ \frac{1}{m} \sum_{i=1}^m (y^*_i-\hat{\beta}^Tx^*_i)^2 \right] \\
        = &  \frac{1}{m} \sum_{i=1}^m E \left[(y^*_i-\hat{\beta}^Tx^*_i)^2 \right]\\
        = &   E \left[(y^*-\hat{\beta}^Tx^*)^2 \right]
     \end{aligned}
    \]

    Therefore, I can choose $m=n$. Now let $\hat{\beta}^{*}$ denote the OLS parameters  
    obtained by fitting a linear model to the test set, and note that the following random variables are identically distributed.

    \[\begin{aligned}
        & \frac{1}{n} \sum_{i=1}^n (y_i-\hat{\beta}^Tx_i)^2 \\
        & \frac{1}{n} \sum_{i=1}^n (y^*_i-\hat{\beta}^{*T}x^*_i)^2 \\
     \end{aligned}
    \]

    Since $\hat{\beta}^{*}$ is defined as:
    \[\arg\min_{\beta}\frac{1}{n} \sum_{i=1}^n (y^*_i-\beta^Tx^*_i)^2 := \arg\min_{\beta} f(x^*, y^*, \beta)\] 
    then the value of $f$ will be larger for any other value of $\beta$ other than $\hat{\beta}^{*T}$.
    Therefore,
    \[
        \begin{aligned}
            E\left[\frac{1}{n} \sum_{i=1}^n (y_i-\hat{\beta}^Tx_i)^2 \right] &\leq E\left[\frac{1}{n} \sum_{i=1}^n (y^*_i-\hat{\beta}^Tx^*_i)^2 \right]\\
            E[MSE_{train}] &\leq E[MSE_{test}]
        \end{aligned}
    \]

    \subsection{A.4}

    (a) 
    
    \[
    \begin{aligned}
        \min_\theta \sum_{i=1}^n(f_\theta(x_i)-y_i)^2\\
        \min_\theta \sum_{i=1}^n(f^2_\theta(x_i)-2f_\theta(x_i)y_i+y_i^2)\\ 
        \min_\theta (nf^2_\theta(x_i)-2f_\theta(x_i)n\bar{y}+\sum_{i=1}^ny_i^2)\\ 
    \end{aligned}
    \]

    Note that terms only containing $y$ do not effect which value of $\theta$ will minimise 
    the expression so they can be removed, or added.

    \[
        \begin{aligned}
            \min_\theta (nf^2_\theta(x_i)-2nf_\theta(x_i)\bar{y}+\bar{y}^2)\\ 
            \min_\theta (f_\theta(x_1)-\bar{y})^2\\
        \end{aligned}
        \]
    This gives a reduced OLS problem.
    
    Note: $\bar{y}=\sum_{i=1}^n \frac{y_i}{n}$.

    (b) The reduced weighted least squares is:

    \[\min_\theta \sum_{i=1}^k {r_i} \left( f_\theta(x_i)-\bar{y}_i\right)^2\]
    

    To show why this is true, divide the summation of the full OLS expression 
    into terms where all the predictor values are the same.

    \[
    \begin{aligned}
        \min_\theta \sum_{i=1}^k \sum_{j=1}^{r_i}(f_\theta(x_i)-y_i^{(j)})^2\\
    \end{aligned}
    \]

    Following the same argument from part (a), we then have

    \[
        \begin{aligned}
            \min_\theta \sum_{i=1}^k r_i(f_\theta(x_i)-\bar{y_i})^2\\
        \end{aligned}
        \]

        which is a reduced, weight least squares problem.

    \section{Part B}

    \subsection{}
    Solutions for part (a) and (b) in the .R file.

    (c) Yes the results of the regression suggest there is a strong relationship between 
    the predictors and the response. In particular, the variables year, origin, weight and displacement 
    are statistically significant predictors of miles per gallon. The fitted coefficient on the year 
    variable tells us that miles per gallon is improving over time.

    (d) There are a few problems. 
    
    \begin{enumerate}
        \item  The residuals vs fitted plot suggests non-independence of errors,  
        since smaller fitted values correspond with positive errors. 
        \item The variance of the residuals is non-constant - it's increasing with the fitted value.
        \item There is also one point that (row 14) which has unusally high leverage. 
        \item The Q-Q shows there's a deviation from the assumption of normal errors.
    \end{enumerate}

    Note however that observation 14 is within the threshold defined by the Cook's distance threshold that is commonly 
    used to identify outliers. \par

    (e) I first considered all possible pair-wise interactions. Under this model the interaction between displacement and 
    weight was statistically significant. I also looked smaller models, which excluded some predictors, to see how the 
    regression would change. Under these reduced models other interactions, such as between horsepower and 
    acceleration, and displacement and weight became significant.

    (f) I found that taking the exponential of the acceleration predictor made it significant.
    Taking the square root of the horsepower variable increased it's significance. I also tried taking the log of
    the response variable and found that this increase the F statistics 1.62 times suggesting 
    a better overall model.

    \subsection{}

    (a)
    $n$ is 100, $p$ is 2. Let $\epsilon \sim N[0,1]$, then the model is

    \[Y = X-2 \cdot X^2 + \epsilon\]

    (b) It looks like a quadratic relationship, values appear more dense around the origin

    (c) The LOOVC errors for the models i-iv are:
    i. 7.288
    ii. 0.937 
    iii. 0.957 
    iv. 0.954

    (d) With the new seed, the errors were the same. This was expected 
    because in the LOOCV procedure all possible subsets of size $n-1$ are 
    considered so the order that they are selected (which is random), does not 
    impacted the minimum error obtained.

    (e) The quadratic model has the smallest error, which was expected as this 
    was precisely the underlying model.

    (f) The statistical significance for the coeffecients are consistent with the 
    conclusions drawn above, p-values for the intercept, first-, and second-order terms are all 
    very small (on the order of $10^{-8}$ or less), whereas coefficient estimates for the third-, 
    and fourth-order terms are much larger, around $10^{-1}$.
 
\end{document}